目录  9
译者序 5
作者简介    4
前言  7
第1章　为什么会有大数据	16
	1.1　什么是大数据	16
	1.2　大数据技术背后的核心思想	17
		1.2.1　把数据分发到多个节点	17
		1.2.2　把计算逻辑移动到数据附近	18
		1.2.3　计算节点进行本地数据处理	18
		1.2.4　优选顺序读，次之随机读	19
		1.2.5　一个例子	19
	1.3　大数据的编程模型	20
		1.3.1　大规模并行处理数据库系统	20
		1.3.2　内存数据库系统	21
		1.3.3　MapReduce系统	21
		1.3.4　整体同步并行系统	23
	1.4　大数据和事务性系统	23
	1.5　我们能处理多大的数据量	24
		1.5.1　一个计算密集型的例子	25
		1.5.2　Amdhal定律	25
	1.6　大数据商业用例	26
	1.7　本章小结	27
第2章　Hadoop中的概念	28
	2.1　Hadoop简介	28
	2.2　MapReduce编程模型简介	30
	2.3　Hadoop系统的组成	34
		2.3.1　Hadoop 分布式文件系统	35
		2.3.2　辅助名称节点	40
		2.3.3　任务跟踪器	41
		2.3.4　作业跟踪器	41
	2.4　Hadoop 2.	42
		2.4.1　容器	44
		2.4.2　节点管理器	44
		2.4.3　资源管理器	45
		2.4.4　应用程序管理器	45
		2.4.5　分步详解YARN请求	46
	2.5　HDFS 的高可用性	48
	2.6　本章小结	48
第3章　初识Hadoop框架	49
	3.1　安装类型	49
		3.1.1　单机模式	50
		3.1.2　伪分布式集群模式	50
		3.1.3　多节点集群安装模式	50
		3.1.4　基于Amazon EMR预安装模式	50
	3.2　使用Cloudera虚拟机搭建开发环境	51
	3.3　一个MapReduce程序的组成	52
	3.4　第一个Hadoop程序	53
		3.4.1　以本地模式运行程序的必要条件	54
		3.4.2　使用旧API编写的单词计数程序	54
		3.4.3　构建程序	57
		3.4.4　在集群模式下运行单词计数程序	57
		3.4.5　使用新API编写的单词计数程序	58
		3.4.6　构建程序	59
		3.4.7　在集群模式下运行单词计数程序	60
	3.5　Hadoop作业中的第三方函数库	60
	3.6　本章小结	65
第4章　Hadoop系统管理	66
	4.1　Hadoop的配置文件	66
	4.2　配置Hadoop守护进程	67
	4.3　Hadoop配置文件的优先级	68
	4.4　深入探究Hadoop配置文件	69
		4.4.1　core—site.xml	69
		4.4.2　hdfs—*.xml	70
		4.4.3　mapred—site.xml	71
		4.4.4　yarn—site.xml	73
		4.4.5　YARN中的内存分配	75
	4.5　调度器	76
		4.5.1　计算能力调度器	77
		4.5.2　公平调度器	80
		4.5.3　公平调度器配置	80
		4.5.4　yarn—site.xml 配置	81
		4.5.5　策略文件的格式和配置	82
		4.5.6　按照drf策略来确定优势资源的分配	83
	4.6　从属文件	84
	4.7　机架感知	84
	4.8　集群管理工具	86
		4.8.1　检查HDFS	86
		4.8.2　HDFS管理命令行	88
		4.8.3　均衡HDFS上的数据分布	90
		4.8.4　从HDFS中复制海量数据	91
	4.9　本章小结	91
第5章　MapReduce开发基础	93
	5.1　Hadoop和数据处理	93
	5.2　航空公司数据集介绍	94
		5.2.1　准备开发环境	95
		5.2.2　准备Hadoop系统	96
	5.3　MapReduce编程模式	96
		5.3.1　只有Map阶段的作业（SELECT和WHERE查询）	97
		5.3.2　问题定义—SELECT子句	97
		5.3.3　问题定义—WHERE子句	105
		5.3.4　Map和Reduce作业（聚合查询）	108
		5.3.5　问题定义—GROUP BY和SUM子句	108
		5.3.6　应用Combiner提高Aggregation性能	114
		5.3.7　问题定义—优化后的Aggregators	114
		5.3.8　Partitioner的作用	119
		5.3.9　问题定义—按月分离航空数据	120
	5.4　综合分析	123
	5.5　本章小结	125
第6章　MapReduce开发进阶	126
	6.1　MapReduce编程模式	126
	6.2　Hadoop I／O 介绍	126
	6.3　问题定义—排序	129
		6.3.1　主要挑战：全排序	130
		6.3.2　在Cluster中运行Sorting作业	140
		6.3.3　仅根据Writable键排序	140
		6.3.4　根据排序回顾Hadoop的关键特性	143
	6.4　问题定义—分析连续的记录	143
		6.4.1　支持二次排序的重要组件	144
		6.4.2　在没有Grouping Comparator的情况下实现Secondary Sort	151
		6.4.3　在Cluster中运行SecondarySort作业	152
		6.4.4　利用Secondary Sort回顾Hadoop的关键特性	152
	6.5　问题定义—使用MapReducer进行连接	153
		6.5.1　处理多输入：Multiple—Inputs 类	153
		6.5.2　具备多个输入的Mapper类	154
		6.5.3　自定义 Partitioner： Carrier—CodeBasedPartioner	156
		6.5.4　在Reducer中实现连接	156
		6.5.5　在集群中运行MapReduce连接作业	158
		6.5.6　探讨与MapReduce相关的Hadoop主要特性	159
	6.6　问题定义—使用Map—Only 作业进行连接	159
		6.6.1　基于DistributeCache的解决方案	160
		6.6.2　在集群中运行Map—Only的连接作业	162
		6.6.3　总结探讨Map—Only连接时的Hadoop关键特性	164
	6.7　在MR作业中保存结果到多输出文件	164
	6.8　使用计数器收集统计数据	166
	6.9　本章小结	168
第7章　Hadoop输入／输出	170
	7.1　压缩方式	170
		7.1.1　压缩内容的选择	171
		7.1.2　各种压缩方式	172
		7.1.3　配置压缩方式	173
	7.2　Hadoop的I／O处理过程内部	174
		7.2.1　Inputformat	174
		7.2.2　OutputFormat	176
		7.2.3　自定义OutputFormat：将文本转换成XML	176
		7.2.4　自定义 InputFormat：使用自定义的XML文件	180
	7.3　Hadoop文件	188
		7.3.1　SequenceFile	188
		7.3.2　MapFiles	193
		7.3.3　Avro Files	195
	7.4　本章小结	200
第8章　测试Hadoop程序	201
	8.1　回顾一下单词统计的程序	201
	8.2　MRUnit概述	203
		8.2.1　安装MRUnit	203
		8.2.2　MRUnit 核心类	203
		8.2.3　编写一个MRUnit测试用例	204
		8.2.4　测试计数器	206
		8.2.5　MRUnit的特性	209
		8.2.6　MRUnit的局限性	209
	8.3　用LocalJobRunner测试	210
		8.3.1　setUp（ ）方法	211
		8.3.2　LocalJobRunner的局限性	212
	8.4　用MiniMRCluster测试	213
		8.4.1　配置开发环境	213
		8.4.2　MiniMRCluster例子	214
		8.4.3　MiniMRCluster的局限性	216
	8.5　对访问网络资源的MR作业进行测试	217
	8.6　本章小结	217
第9章　Hadoop的监控	218
	9.1　在Hadoop MapReduce Jobs中写日志消息	218
	9.2　在Hadoop MapReduce Jobs中查看日志消息	221
	9.3　在Hadoop 2.x中使用日志管理	223
		9.3.1　Hadoop 2.x中的日志存储	223
		9.3.2　日志管理提升	225
		9.3.3　使用基于Web的界面查看日志	225
		9.3.4　命令行界面	226
		9.3.5　日志的保存	226
	9.4　Hadoop集群性能监控	226
	9.5　使用YARN REST API	227
	9.6　使用供应商工具管理Hadoop集群	228
	9.7　本章小结	229
第10章　使用Hadoop构建数据仓库	230
	10.1　Apache Hive	230
		10.1.1　安装Hive	231
		10.1.2　Hive的架构	232
		10.1.3　元数据存储	232
		10.1.4　HiveQL编译基础	232
		10.1.5　Hive使用的概念	233
		10.1.6　HiveQL编译细节	237
		10.1.7　数据定义语言	241
		10.1.8　数据操作语言	241
		10.1.9　扩展接口	242
		10.1.10　Hive脚本	244
		10.1.11　性能表现	244
		10.1.12　整合MapReduce	245
		10.1.13　创建分区	245
		10.1.14　用户定义函数	247
	10.2　Impala	249
		10.2.1　Impala架构	249
		10.2.2　Impala特性	250
		10.2.3　Impala的局限	250
	10.3　Shark	250
	10.4　本章小结	252
第11章　使用Pig进行数据处理	253
	11.1　Pig简介	253
	11.2　运行Pig	255
		11.2.1　在Grunt Shell中执行	256
		11.2.2　执行Pig脚本	256
		11.2.3　嵌入式Java程序	257
	11.3　Pig Latin	258
		11.3.1　Pig脚本中的注释	258
		11.3.2　Pig语句的执行	258
		11.3.3　Pig命令	259
	11.4　UDF	264
		11.4.1　Mapper中的Eval函数调用	264
		11.4.2　Reducer中的Eval函数调用	265
		11.4.3　编写并使用自定义Filter—Func	271
	11.5　Pig与Hive对比	273
	11.6　Crunch API	274
		11.6.1　Crunch与Pig的区别	274
		11.6.2　Crunch管道的例子	275
	11.7　本章小结	280
第12章　HCatalog和企业级Hadoop	281
	12.1　HCataolg和企业级数据仓库用户	281
	12.2　HCatalog技术背景简介	282
		12.2.1　HCatalog命令行接口	284
		12.2.2　WebHCat	284
		12.2.3　HCatalog的MapReduce接口	285
		12.2.4　HCatalog的Pig接口	288
		12.2.5　HCatalog通知接口	289
	12.3　HCatalog的安全和认证机制	289
	12.4　完整的解决方案	290
	12.5　本章小结	290
第13章　使用Hadoop分析日志	292
	13.1　日志文件分析应用	292
		13.1.1　网络分析	292
		13.1.2　安全规范与法务	293
		13.1.3　监控和报警	294
		13.1.4　物联网	294
	13.2　分析步骤	295
		13.2.1　载入	295
		13.2.2　提取	295
		13.2.3　可视化	296
	13.3　Apache Flume	296
	13.4　Netflix Suro	298
	13.5　云解决方案	300
	13.6　本章小结	300
第14章　使用HBase构建实时系统	301
	14.1　HBase是什么	301
	14.2　典型的HBase用例场景	302
	14.3　HBase数据模型	303
		14.3.1　HBase逻辑视图和客户端视图	303
		14.3.2　HBase与RDBMS的区别	304
		14.3.3　HBase表	305
		14.3.4　HBase单元格	305
		14.3.5　HBase列簇	305
	14.4　HBase命令和API	306
		14.4.1　获取命令列表：帮助命令	306
		14.4.2　创建表：create命令	307
		14.4.3　向表中加入行：put命令	308
		14.4.4　从表中检索行：get命令	308
		14.4.5　读取多行：scan命令	308
		14.4.6　统计表中的行数：count命令	308
		14.4.7　删除行：delete命令	309
		14.4.8　清空表：truncate命令	309
		14.4.9　删除表：drop命令	309
		14.4.10　更换表：alter命令	309
	14.5　HBase架构	310
		14.5.1　HBase组件	310
		14.5.2　HBase中的压缩与分区	317
		14.5.3　压缩	318
	14.6　HBase配置概览	319
	14.7　HBase应用程序设计	320
		14.7.1　长表vs宽表vs窄表	320
		14.7.2　行键设计	321
	14.8　使用Java API操作HBase	322
		14.8.1　一切都是字节	322
		14.8.2　创建HBase表	322
		14.8.3　使用HBaseAdmin类管理HBase	323
		14.8.4　使用Java API访问数据	323
	14.9　HBase与MapReduce集成	327
		14.9.1　使用MapReduce任务读取HBase表	327
		14.9.2　HBase和MapReduce集群	330
	14.10　本章小结	331
第15章　Hadoop与数据科学	332
	15.1　Hadoop中的数据科学方法	333
	15.2　Apache Hama	333
		15.2.1　整体同步并行计算模型	333
		15.2.2　Hama Hello World！	334
		15.2.3　蒙特卡洛方法	336
		15.2.4　K—Means聚类	339
	15.3　Apache Spark	342
		15.3.1　弹性分布式数据集（RDD）	342
		15.3.2　Spark与蒙特卡洛算法	343
		15.3.3　Spark与KMeans聚类	345
	15.4　RHadoop	347
	15.5　本章小结	348
第16章　Hadoop与云计算	349
	16.1　经济性	349
		16.1.1　自有集群	350
		16.1.2　基于云平台的集群	350
		16.1.3　弹性	351
		16.1.4　按需付费	351
		16.1.5　竞价	351
		16.1.6　混合集群	351
	16.2　后勤保障	352
		16.2.1　导入／导出	352
		16.2.2　数据保存	352
	16.3　安全性	352
	16.4　云端应用模型	353
	16.5　云服务商	354
		16.5.1　亚马逊网络服务（AWS）	354
		16.5.2　谷歌云平台	356
		16.5.3　微软Azure	357
		16.5.4　选择云服务商	357
	16.6　案例学习： AWS	357
		16.6.1　EMR	358
		16.6.2　EC	2360
	16.7　本章小结	363
第17章　构建YARN应用程序	364
	17.1　YARN：通用分布式系统	364
	17.2　YARN：快速浏览	366
	17.3　创建YARN应用程序	368
	17.4　DownloadService.java类	369
	17.5　Client.java类	371
		17.5.1　从客户端启动应用管理器的步骤	371
		17.5.2　创建YarnClient	372
		17.5.3　配置应用程序	372
		17.5.4　启动应用管理器	375
		17.5.5　监控应用	375
	17.6　ApplicationMaster.java	377
		17.6.1　启动工作任务的步骤	378
		17.6.2　初始化应用管理器协议和容器管理协议	379
		17.6.3　在资源管理器中注册应用管理器	379
		17.6.4　配置容器参数	379
		17.6.5　向资源管理器请求容器	379
		17.6.6　在任务节点上启动容器	379
		17.6.7　等待容器结束工作任务	380
		17.6.8　在资源管理器中注销应用管理器	380
	17.7　运行应用管理器	382
		17.7.1　在非托管模式中启动应用管理器	382
		17.7.2　在托管模式中启动应用管理器	382
	17.8　本章小结	382
附录Ａ安装Hadoop　384
附录B使用Maven和Eclipse　393
附录CApache Ambari　398